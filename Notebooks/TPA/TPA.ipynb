{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e6a64d",
   "metadata": {},
   "source": [
    "# TPA\n",
    "\n",
    "[Temporal Pattern Attention for Multivariate Time Series Forecasting](https://arxiv.org/abs/1809.04206)\n",
    "\n",
    "## 1 Introduction\n",
    "However, one disadvantage in using RNNs in time series analysis is their weakness on managing long-term dependencies.\n",
    "\n",
    "The main contributions of this paper are summarized as follows:\n",
    "\n",
    "- We introduce a new attention concept in which we select the relevant variables as opposed to the relevant time steps. The method is simple and general to apply on RNN.\n",
    "- We use toy examples to verify that our attention mechanism enables the model to extract temporal patterns and focus on di\u000b",
    "erent time steps for different time series\n",
    "- Attested by experimental results on real-world data ranging from periodic and partially linear to non-periodic and non-linear tasks, we show that the proposed attention mechanism achieves state-of-the-art results across multiple datasets.\n",
    "- The learned CNN filters in our attention mechanism demonstrate interesting and interpretable behavior.\n",
    "\n",
    "## 2 Related Work\n",
    "LSTNet has three major shortcomings when compared to our proposed attention mechanism:\n",
    "- the skip length of the recurrent-skip layer must be manually tuned in order to match the period of the data, whereas our proposed approach learns the periodic patterns by itself;\n",
    "- the LSTNet-Skip model is specifically designed for MTS data with periodic patterns, whereas our proposed model, as shown in our experiments, is simple and adaptable to various datasets, even non-periodic ones\n",
    "- the attention layer in LSTNet-Attn model selects a relevant hidden state as in typical attention mechanism, whereas our proposed attention mechanism selects relevant time series which is a more suitable mechanism for MTS data.\n",
    "\n",
    "## 4 Temporal Pattern Attention\n",
    "\n",
    "### 4.2 Temporal Pattern Detection using CNN\n",
    "\n",
    "### 4.3 Proposed Attention Mechanism\n",
    "![img](01.JPG)\n",
    "![img](02.JPG)\n",
    "![img](03.JPG)\n",
    "![img](04.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a058d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
